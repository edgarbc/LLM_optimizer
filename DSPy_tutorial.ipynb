{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOAqipV8HVicp/VzdVuxLUk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/edgarbc/LLM_optimizer/blob/main/DSPy_tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tutorial to demonstrate how to use DSPy for prompt engineer.\n",
        "\n",
        "By Edgar Bermudez - github: edgarbc\n",
        "\n",
        "Based on https://learnbybuilding.ai/tutorials/a-gentle-introduction-to-dspy\n",
        "\n",
        "\n",
        "\n",
        "June, 2024"
      ],
      "metadata": {
        "id": "ZMNdbNHWfc5y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install dspy\n",
        "!pip install openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jmiX4BhOko4U",
        "outputId": "08c0a2ec-98e1-42ad-f8e0-1bf22e65ef87"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting dspy\n",
            "  Downloading dspy-0.1.5-py3-none-any.whl (1.3 kB)\n",
            "Collecting dspy-ai==2.4.5 (from dspy)\n",
            "  Downloading dspy_ai-2.4.5-py3-none-any.whl (197 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m197.5/197.5 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting backoff~=2.2.1 (from dspy-ai==2.4.5->dspy)\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Collecting joblib~=1.3.2 (from dspy-ai==2.4.5->dspy)\n",
            "  Downloading joblib-1.3.2-py3-none-any.whl (302 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.2/302.2 kB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting openai<2.0.0,>=0.28.1 (from dspy-ai==2.4.5->dspy)\n",
            "  Downloading openai-1.35.13-py3-none-any.whl (328 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m328.5/328.5 kB\u001b[0m \u001b[31m30.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from dspy-ai==2.4.5->dspy) (2.0.3)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from dspy-ai==2.4.5->dspy) (2024.5.15)\n",
            "Collecting ujson (from dspy-ai==2.4.5->dspy)\n",
            "  Downloading ujson-5.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.6/53.6 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from dspy-ai==2.4.5->dspy) (4.66.4)\n",
            "Collecting datasets<3.0.0,~=2.14.6 (from dspy-ai==2.4.5->dspy)\n",
            "  Downloading datasets-2.14.7-py3-none-any.whl (520 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m520.4/520.4 kB\u001b[0m \u001b[31m41.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from dspy-ai==2.4.5->dspy) (2.31.0)\n",
            "Collecting optuna (from dspy-ai==2.4.5->dspy)\n",
            "  Downloading optuna-3.6.1-py3-none-any.whl (380 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m380.1/380.1 kB\u001b[0m \u001b[31m29.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pydantic==2.5.0 (from dspy-ai==2.4.5->dspy)\n",
            "  Downloading pydantic-2.5.0-py3-none-any.whl (407 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m407.5/407.5 kB\u001b[0m \u001b[31m25.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic==2.5.0->dspy-ai==2.4.5->dspy) (0.7.0)\n",
            "Collecting pydantic-core==2.14.1 (from pydantic==2.5.0->dspy-ai==2.4.5->dspy)\n",
            "  Downloading pydantic_core-2.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m50.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic==2.5.0->dspy-ai==2.4.5->dspy) (4.12.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets<3.0.0,~=2.14.6->dspy-ai==2.4.5->dspy) (1.25.2)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets<3.0.0,~=2.14.6->dspy-ai==2.4.5->dspy) (14.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets<3.0.0,~=2.14.6->dspy-ai==2.4.5->dspy) (0.6)\n",
            "Collecting dill<0.3.8,>=0.3.0 (from datasets<3.0.0,~=2.14.6->dspy-ai==2.4.5->dspy)\n",
            "  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting xxhash (from datasets<3.0.0,~=2.14.6->dspy-ai==2.4.5->dspy)\n",
            "  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess (from datasets<3.0.0,~=2.14.6->dspy-ai==2.4.5->dspy)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets<3.0.0,~=2.14.6->dspy-ai==2.4.5->dspy) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets<3.0.0,~=2.14.6->dspy-ai==2.4.5->dspy) (3.9.5)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from datasets<3.0.0,~=2.14.6->dspy-ai==2.4.5->dspy) (0.23.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets<3.0.0,~=2.14.6->dspy-ai==2.4.5->dspy) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets<3.0.0,~=2.14.6->dspy-ai==2.4.5->dspy) (6.0.1)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=0.28.1->dspy-ai==2.4.5->dspy) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai<2.0.0,>=0.28.1->dspy-ai==2.4.5->dspy) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai<2.0.0,>=0.28.1->dspy-ai==2.4.5->dspy)\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=0.28.1->dspy-ai==2.4.5->dspy) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->dspy-ai==2.4.5->dspy) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->dspy-ai==2.4.5->dspy) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->dspy-ai==2.4.5->dspy) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->dspy-ai==2.4.5->dspy) (2024.7.4)\n",
            "Collecting alembic>=1.5.0 (from optuna->dspy-ai==2.4.5->dspy)\n",
            "  Downloading alembic-1.13.2-py3-none-any.whl (232 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.0/233.0 kB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting colorlog (from optuna->dspy-ai==2.4.5->dspy)\n",
            "  Downloading colorlog-6.8.2-py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: sqlalchemy>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from optuna->dspy-ai==2.4.5->dspy) (2.0.31)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->dspy-ai==2.4.5->dspy) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->dspy-ai==2.4.5->dspy) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->dspy-ai==2.4.5->dspy) (2024.1)\n",
            "Collecting Mako (from alembic>=1.5.0->optuna->dspy-ai==2.4.5->dspy)\n",
            "  Downloading Mako-1.3.5-py3-none-any.whl (78 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=0.28.1->dspy-ai==2.4.5->dspy) (1.2.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<3.0.0,~=2.14.6->dspy-ai==2.4.5->dspy) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<3.0.0,~=2.14.6->dspy-ai==2.4.5->dspy) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<3.0.0,~=2.14.6->dspy-ai==2.4.5->dspy) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<3.0.0,~=2.14.6->dspy-ai==2.4.5->dspy) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<3.0.0,~=2.14.6->dspy-ai==2.4.5->dspy) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<3.0.0,~=2.14.6->dspy-ai==2.4.5->dspy) (4.0.3)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai<2.0.0,>=0.28.1->dspy-ai==2.4.5->dspy)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=0.28.1->dspy-ai==2.4.5->dspy)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets<3.0.0,~=2.14.6->dspy-ai==2.4.5->dspy) (3.15.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->dspy-ai==2.4.5->dspy) (1.16.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy>=1.3.0->optuna->dspy-ai==2.4.5->dspy) (3.0.3)\n",
            "INFO: pip is looking at multiple versions of multiprocess to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting multiprocess (from datasets<3.0.0,~=2.14.6->dspy-ai==2.4.5->dspy)\n",
            "  Downloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.10/dist-packages (from Mako->alembic>=1.5.0->optuna->dspy-ai==2.4.5->dspy) (2.1.5)\n",
            "Installing collected packages: xxhash, ujson, pydantic-core, Mako, joblib, h11, dill, colorlog, backoff, pydantic, multiprocess, httpcore, alembic, optuna, httpx, openai, datasets, dspy-ai, dspy\n",
            "  Attempting uninstall: pydantic-core\n",
            "    Found existing installation: pydantic_core 2.20.1\n",
            "    Uninstalling pydantic_core-2.20.1:\n",
            "      Successfully uninstalled pydantic_core-2.20.1\n",
            "  Attempting uninstall: joblib\n",
            "    Found existing installation: joblib 1.4.2\n",
            "    Uninstalling joblib-1.4.2:\n",
            "      Successfully uninstalled joblib-1.4.2\n",
            "  Attempting uninstall: pydantic\n",
            "    Found existing installation: pydantic 2.8.2\n",
            "    Uninstalling pydantic-2.8.2:\n",
            "      Successfully uninstalled pydantic-2.8.2\n",
            "Successfully installed Mako-1.3.5 alembic-1.13.2 backoff-2.2.1 colorlog-6.8.2 datasets-2.14.7 dill-0.3.7 dspy-0.1.5 dspy-ai-2.4.5 h11-0.14.0 httpcore-1.0.5 httpx-0.27.0 joblib-1.3.2 multiprocess-0.70.15 openai-1.35.13 optuna-3.6.1 pydantic-2.5.0 pydantic-core-2.14.1 ujson-5.10.0 xxhash-3.4.1\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (1.35.13)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.27.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.5.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.7.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.5)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.14.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.14.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "3tE3W5AwfXKq"
      },
      "outputs": [],
      "source": [
        "# installs\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import dspy"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load parameters. Google colab now allows you to define secrets and load them. Choose the key symbol from the navigation bar to the left."
      ],
      "metadata": {
        "id": "tLNWMrs-kqMB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "import os\n",
        "api_key = userdata.get('OPENAI_API_KEY')\n",
        "\n",
        "# Set the API key as an environment variable\n",
        "os.environ[\"OPENAI_API_KEY\"] = api_key"
      ],
      "metadata": {
        "id": "Bbj4CEuVgZC-"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Data extraction\n",
        "Get some grug brain developer text from html using beautifulsoup. Check https://grugbrain.dev"
      ],
      "metadata": {
        "id": "WmMfydH3WiE5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "res = requests.get(\"https://grugbrain.dev/\")\n",
        "soup = BeautifulSoup(res.text, 'html.parser')\n",
        "raw_text = [p.text for p in soup.find_all('p') if p.text]"
      ],
      "metadata": {
        "id": "q72dlnsRgSx1"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# show some raw text examples\n",
        "raw_text[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EN6_tBHrlgcA",
        "outputId": "317266c8-d774-4b69-d1ae-02887a809893"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['this collection of thoughts on software development gathered by grug brain developer',\n",
              " 'grug brain developer not so smart, but grug brain developer program many long year and learn some things\\nalthough mostly still confused',\n",
              " 'grug brain developer try collect learns into small, easily digestible and funny page, not only for you, the young grug, but also for him\\nbecause as grug brain developer get older he forget important things, like what had for breakfast or if put pants on',\n",
              " 'big brained developers are many, and some not expected to like this, make sour face',\n",
              " 'THINK they are big brained developers many, many more, and more even definitely probably maybe not like this, many\\nsour face (such is internet)']"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. LLM setup\n",
        "Setup the chatgpt 3.5 and the class to handle data and translate grudge language into plain English."
      ],
      "metadata": {
        "id": "4xlpwvsvW4TG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "# initialize an openai client\n",
        "\n",
        "client = OpenAI()\n",
        "openai_model_name= \"gpt-3.5-turbo\"\n",
        "\n",
        "class BuildMessages:\n",
        "    def __init__(self, system_prompt, user_prompt):\n",
        "        self.system_prompt = system_prompt\n",
        "        self.user_prompt = user_prompt\n",
        "    def render(self, **kwargs):\n",
        "        sys = self.system_prompt.format(**kwargs)\n",
        "        user = self.user_prompt.format(**kwargs)\n",
        "        return [\n",
        "            {\"role\":\"system\", \"content\":sys},\n",
        "            {\"role\":\"user\", \"content\":user},\n",
        "        ]\n",
        "from functools import cache\n",
        "@cache\n",
        "def translate_grug(grug_text):\n",
        "    prompt = BuildMessages(\n",
        "    \"You are an expert in deciphering strange text. The user will provide text written by someone named Grug and you will provide the translation.\",\n",
        "    \"\"\"Translate the following text into plain english: '{text}'.\n",
        "\n",
        "    Do not respond with any other text. Only provide that text. Now take a deep breath and begin.\"\"\"\n",
        ")\n",
        "    result = client.chat.completions.create(messages=prompt.render(text=grug_text), model=openai_model_name)\n",
        "    return result.choices[0].message.content"
      ],
      "metadata": {
        "id": "s_kEl2VugfSU"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now translate 10 examples using the translating function"
      ],
      "metadata": {
        "id": "wJsw3CcNaJJZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = []\n",
        "for grug_text in raw_text[:10]:\n",
        "    translated = translate_grug(grug_text)\n",
        "    dataset.append({\"grug_text\":grug_text, \"plain_english\":translated})"
      ],
      "metadata": {
        "id": "UaJO5yJtg8BS"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now construct DSPy examples using the translated dataset above."
      ],
      "metadata": {
        "id": "6gTyRNTNa2D_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "examples = []\n",
        "for row in dataset:\n",
        "    examples.append(dspy.Example(grug_text=row[\"grug_text\"], plain_english=row[\"plain_english\"]).with_inputs(\"plain_english\"))"
      ],
      "metadata": {
        "id": "q-fKZVAahGDL"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(examples)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fx0jOiGPbLQf",
        "outputId": "95603dc5-a150-41d5-e0eb-804f311a84c9"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Example({'grug_text': 'this collection of thoughts on software development gathered by grug brain developer', 'plain_english': 'Grug is a software developer who has collected thoughts on software development.'}) (input_keys={'plain_english'}), Example({'grug_text': 'grug brain developer not so smart, but grug brain developer program many long year and learn some things\\nalthough mostly still confused', 'plain_english': 'Grug is not very smart, but Grug has been developing programs for many years and has learned some things. However, Grug is still mostly confused.'}) (input_keys={'plain_english'}), Example({'grug_text': 'grug brain developer try collect learns into small, easily digestible and funny page, not only for you, the young grug, but also for him\\nbecause as grug brain developer get older he forget important things, like what had for breakfast or if put pants on', 'plain_english': 'Grug, as a brain developer, tries to collect learning into small, easily digestible, and funny pages. These are not only for you, the young Grug, but also for him. As Grug, the brain developer, gets older, he forgets important things, like what he had for breakfast or if he put pants on.'}) (input_keys={'plain_english'}), Example({'grug_text': 'big brained developers are many, and some not expected to like this, make sour face', 'plain_english': 'Smart developers are plentiful, and some may not be expected to enjoy this, resulting in a displeased expression.'}) (input_keys={'plain_english'}), Example({'grug_text': 'THINK they are big brained developers many, many more, and more even definitely probably maybe not like this, many\\nsour face (such is internet)', 'plain_english': 'Grug thinks there are many developers who believe they are smart, but there are even more who probably, definitely, or maybe are not like that. This often leads to a sour expression (such is the internet).'}) (input_keys={'plain_english'}), Example({'grug_text': '(note: grug once think big brained but learn hard way)', 'plain_english': 'Grug used to think he was very intelligent, but he learned the hard way that he was not as smart as he thought.'}) (input_keys={'plain_english'}), Example({'grug_text': 'is fine!', 'plain_english': 'That text appears to already be in plain English and does not require translation.'}) (input_keys={'plain_english'}), Example({'grug_text': 'is free country sort of and end of day not really matter too much, but grug hope you fun reading and maybe learn from\\nmany, many mistake grug make over long program life', 'plain_english': \"This country allows for some freedom but at the end of the day, it doesn't really matter much. However, Grug hopes you enjoy reading and maybe learn from the many mistakes Grug has made throughout his long life in programming.\"}) (input_keys={'plain_english'}), Example({'grug_text': 'apex predator of grug is complexity', 'plain_english': \"Grug's ultimate predator is complexity.\"}) (input_keys={'plain_english'}), Example({'grug_text': 'complexity bad', 'plain_english': '\"Complexity is bad.\"'}) (input_keys={'plain_english'})]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Training dataset preparation"
      ],
      "metadata": {
        "id": "5z7f-N1Fbp0l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from random import shuffle\n",
        "def split_for_train_test(values, test_size = 1/3.0):\n",
        "    shuffle(values)\n",
        "    train = int(len(values)-test_size*len(values))\n",
        "    print(train)\n",
        "    return values[:train], values[train:]\n",
        "train, test = split_for_train_test(examples)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aHDo2q1NhJ8B",
        "outputId": "fd0e4bb9-f8f2-4ed0-e59a-c1bf35b89641"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vsofTiVLhR0b",
        "outputId": "436802b3-e381-4051-80a4-dbe954660c94"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Example({'grug_text': '(note: grug once think big brained but learn hard way)', 'plain_english': 'Grug used to think he was very intelligent, but he learned the hard way that he was not as smart as he thought.'}) (input_keys={'plain_english'})"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. DSPy setup\n",
        "Define signatures for the translation task"
      ],
      "metadata": {
        "id": "PMQuYh4BdZZw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import dspy\n",
        "class GrugTranslation(dspy.Signature):\n",
        "    \"Translate plain english to Grug text.\"\n",
        "    plain_english = dspy.InputField()\n",
        "    grug_text = dspy.OutputField()"
      ],
      "metadata": {
        "id": "m6-_rlQGhfJz"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define the model\n",
        "turbo = dspy.OpenAI(model='gpt-3.5-turbo', max_tokens=1000)\n",
        "# config settings\n",
        "dspy.settings.configure(lm=turbo)\n",
        "# define the signature for the grug translation function\n",
        "from dspy.signatures.signature import signature_to_template\n",
        "grug_translation_as_template = signature_to_template(GrugTranslation)\n"
      ],
      "metadata": {
        "id": "rbSpNW_KhicH"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"DSPy template: {str(grug_translation_as_template)}\")\n",
        "print(f\"Translation test: {grug_translation_as_template.query(examples[0])}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zHIVVVkCeEVs",
        "outputId": "e6d5cd02-dcbb-40cf-825c-e9f09d334cd1"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DSPy template: Template(Translate plain english to Grug text., ['Plain English:', 'Grug Text:'])\n",
            "Translation test: Plain English: Grug used to think he was very intelligent, but he learned the hard way that he was not as smart as he thought.\n",
            "Grug Text: (note: grug once think big brained but learn hard way)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now lets see the signature in the class (inherited from DSPy signature)"
      ],
      "metadata": {
        "id": "_Q6uaR7pfEjj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "GrugTranslation.signature\n",
        "GrugTranslation.with_instructions"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "id": "9VUhutcGeGh-",
        "outputId": "13bff7e9-e50b-4f50-f2ff-b4942e95a29c"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<bound method SignatureMeta.with_instructions of GrugTranslation(plain_english -> grug_text\n",
              "    instructions='Translate plain english to Grug text.'\n",
              "    plain_english = Field(annotation=str required=True json_schema_extra={'__dspy_field_type': 'input', 'prefix': 'Plain English:', 'desc': '${plain_english}'})\n",
              "    grug_text = Field(annotation=str required=True json_schema_extra={'__dspy_field_type': 'output', 'prefix': 'Grug Text:', 'desc': '${grug_text}'})\n",
              ")>"
            ],
            "text/html": [
              "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
              "      pre.function-repr-contents {\n",
              "        overflow-x: auto;\n",
              "        padding: 8px 12px;\n",
              "        max-height: 500px;\n",
              "      }\n",
              "\n",
              "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
              "        cursor: pointer;\n",
              "        max-height: 100px;\n",
              "      }\n",
              "    </style>\n",
              "    <pre style=\"white-space: initial; background:\n",
              "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
              "         border-bottom: 1px solid var(--colab-border-color);\"><b>dspy.signatures.signature.SignatureMeta.with_instructions</b><br/>def with_instructions(cls, instructions: str) -&gt; Type[&#x27;Signature&#x27;]</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/usr/local/lib/python3.10/dist-packages/dspy/signatures/signature.py</a>&lt;no docstring&gt;</pre>\n",
              "      <script>\n",
              "      if (google.colab.kernel.accessAllowed && google.colab.files && google.colab.files.view) {\n",
              "        for (const element of document.querySelectorAll('.filepath')) {\n",
              "          element.style.display = 'block'\n",
              "          element.onclick = (event) => {\n",
              "            event.preventDefault();\n",
              "            event.stopPropagation();\n",
              "            google.colab.files.view(element.textContent, 93);\n",
              "          };\n",
              "        }\n",
              "      }\n",
              "      for (const element of document.querySelectorAll('.function-repr-contents')) {\n",
              "        element.onclick = (event) => {\n",
              "          event.preventDefault();\n",
              "          event.stopPropagation();\n",
              "          element.classList.toggle('function-repr-contents-collapsed');\n",
              "        };\n",
              "      }\n",
              "      </script>\n",
              "      </div>"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define the DSPy module to carry out the prompt engineering technique"
      ],
      "metadata": {
        "id": "HM0k5QXuhCRY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CoT(dspy.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.prog = dspy.ChainOfThought(GrugTranslation)\n",
        "\n",
        "    def forward(self, plain_english):\n",
        "        return self.prog(plain_english=plain_english)\n",
        "c = CoT()"
      ],
      "metadata": {
        "id": "siedPJBlidw1"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "c.forward(\"You should not construct complex systems.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Etuhe_8xiiQH",
        "outputId": "e796ee83-06f0-4fe7-c2fc-09c484d7b677"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Prediction(\n",
              "    rationale='produce the grug_text. We want to simplify things and avoid unnecessary complications.',\n",
              "    grug_text='You no make big big thing.'\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Metrics\n",
        "Define readibility index"
      ],
      "metadata": {
        "id": "oWt8EoWYh_Ep"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# https://apps.dtic.mil/sti/tr/pdf/AD0667273.pdf\n",
        "def automated_readability_index(text):\n",
        "    import re\n",
        "    characters = len(re.sub(r'\\s+', '', text)) # Count characters (ignoring whitespace)\n",
        "    words = len(text.split()) # Count words by splitting the text\n",
        "    # Count sentences by finding period, exclamation, or question mark\n",
        "    sentences = len(re.findall(r'[.!?\\n]', text))\n",
        "    # small change is to add a new line character as grug doesn't seem to use punctuation.\n",
        "    if words == 0 or sentences == 0:  # Prevent division by zero\n",
        "        return 0\n",
        "    # Calculate the Automated Readability Index (ARI)\n",
        "    ari = (4.71 * (characters / words)) + (0.5 * (words / sentences)) - 21.43\n",
        "\n",
        "    return round(ari, 2)"
      ],
      "metadata": {
        "id": "rIkA4nTninX9"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test the automated readibility index (ARI) on the examples."
      ],
      "metadata": {
        "id": "ImtqBycwiOEP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"    Eng    \\tGrug\")\n",
        "for ex in examples:\n",
        "    source_ari = automated_readability_index(ex.plain_english)\n",
        "    grug_ari = automated_readability_index(ex.grug_text)\n",
        "    print(f\"ARI {source_ari} \\t=> {grug_ari}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mn2V_h7fitLM",
        "outputId": "06bfb7a0-1c99-473e-c097-475555566d7d"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    Eng    \tGrug\n",
            "ARI 8.3 \t=> 0\n",
            "ARI 8.78 \t=> -3.95\n",
            "ARI 8.33 \t=> 0\n",
            "ARI 11.65 \t=> 0\n",
            "ARI 8.7 \t=> 22.95\n",
            "ARI 12.69 \t=> 0\n",
            "ARI 10.59 \t=> 14.62\n",
            "ARI 9.81 \t=> 14.12\n",
            "ARI 14.04 \t=> 0\n",
            "ARI 7.62 \t=> 13.98\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "LLM-as-a-judge for automated evaluation"
      ],
      "metadata": {
        "id": "nvA0G0vZjBv9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# https://dspy-docs.vercel.app/docs/building-blocks/metrics#intermediate-using-ai-feedback-for-your-metric\n",
        "class AssessBasedOnQuestion(dspy.Signature):\n",
        "    \"\"\"Given the assessed text provide a yes or no to the assessment question.\"\"\"\n",
        "    assessed_text = dspy.InputField(format=str)\n",
        "    assessment_question = dspy.InputField(format=str)\n",
        "    assessment_answer = dspy.OutputField(desc=\"Yes or No\")"
      ],
      "metadata": {
        "id": "OxGFe3a5iwSA"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example_question_assessment = dspy.Example(assessed_text=\"This is a test.\", assessment_question=\"Is this a test?\", assessment_answer=\"Yes\").with_inputs(\"assessed_text\", \"assessment_question\")\n",
        "print(signature_to_template(AssessBasedOnQuestion).query(example_question_assessment))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0C3HcxsMizuk",
        "outputId": "06fd79f9-edf6-4a65-a58f-41455e8a374c"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Assessed Text: This is a test.\n",
            "Assessment Question: Is this a test?\n",
            "Assessment Answer: Yes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The judge LLM will be GPT4.\n",
        "The judge will decide whether the meaning of the translated and original texts are the same (similarity metric).\n"
      ],
      "metadata": {
        "id": "8bbzTrzjkBy4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gpt4T = dspy.OpenAI(model='gpt-4-turbo', max_tokens=500)\n",
        "def similarity_metric(truth, pred, trace=None):\n",
        "    truth_grug_text = truth.grug_text\n",
        "    proposed_grug_text = pred.grug_text\n",
        "    similarity_question = f\"\"\"Does the assessed text have the same meaning as the gold_standard text provided?\n",
        "Gold Standard: \"{truth_grug_text}\"\n",
        "Provide only a yes or no answer.\"\"\"\n",
        "    with dspy.context(lm=gpt4T):\n",
        "        assessor = dspy.Predict(AssessBasedOnQuestion)\n",
        "        raw_similarity_result = assessor(assessed_text=proposed_grug_text, assessment_question=similarity_question)\n",
        "    print(raw_similarity_result) # for debugging\n",
        "    raw_similarity = raw_similarity_result.assessment_answer.lower().strip()\n",
        "    same_meaning = raw_similarity == 'yes'\n",
        "    return same_meaning"
      ],
      "metadata": {
        "id": "HHBTwECNi4XW"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ari_metric(truth, pred, trace=None):\n",
        "    truth_grug_text = truth.grug_text\n",
        "    proposed_grug_text = pred.grug_text\n",
        "\n",
        "    gold_ari = automated_readability_index(truth_grug_text)\n",
        "    pred_ari = automated_readability_index(proposed_grug_text)\n",
        "    print(f\"ARI {gold_ari} => {pred_ari}\")\n",
        "    ari_result = pred_ari <= 7.01\n",
        "    return ari_result"
      ],
      "metadata": {
        "id": "PsO8RRfKi95N"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define the overall metric, if the meaning of the translated and source are similar and the automated readibility index is below 7.0."
      ],
      "metadata": {
        "id": "WxUR_tJVow6a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def overall_metric(provided_example, predicted, trace=None):\n",
        "    similarity = similarity_metric(provided_example, predicted, trace)\n",
        "    ari = ari_metric(provided_example, predicted, trace)\n",
        "    if similarity and ari:\n",
        "        return True\n",
        "    return False"
      ],
      "metadata": {
        "id": "5aUL-j5FjBHy"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7. Prompt optimization\n"
      ],
      "metadata": {
        "id": "Nfe1AIqHkbHH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# run optimization\n",
        "from dspy.teleprompt import BootstrapFewShot\n",
        "config = dict(max_bootstrapped_demos=4, max_labeled_demos=4)\n",
        "optimizer = BootstrapFewShot(metric=overall_metric, **config)\n",
        "optimizer.max_errors = 1 # helpful to debug errors faster\n",
        "optimized_cot = optimizer.compile(CoT(), trainset=train, valset=test)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mfN6gAyIjEjq",
        "outputId": "70f7f483-fdfe-496f-b391-2ca5a91b3334"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 17%|█▋        | 1/6 [00:04<00:22,  4.51s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction(\n",
            "    assessment_answer='Assessed Text: grug think smart, but find out not as smart as think\\nAssessment Question: Does the assessed text have the same meaning as the gold_standard text provided?\\nGold Standard: \"(note: grug once think big brained but learn hard way)\"\\nAssessment Answer: Yes'\n",
            ")\n",
            "ARI 0 => 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 33%|███▎      | 2/6 [00:06<00:13,  3.29s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction(\n",
            "    assessment_answer='Assessed Text: That text appears to already be in plain English and does not require translation.\\nAssessment Question: Does the assessed text have the same meaning as the gold_standard text provided?\\nGold Standard: \"is fine!\"\\nAssessment Answer: Yes'\n",
            ")\n",
            "ARI -3.95 => 8.78\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 50%|█████     | 3/6 [00:09<00:08,  2.92s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction(\n",
            "    assessment_answer='Assessed Text: \"Hard stuff not good.\"\\nAssessment Question: Does the assessed text have the same meaning as the gold_standard text provided?\\nGold Standard: \"complexity bad\"\\nAssessment Answer: Yes'\n",
            ")\n",
            "ARI 0 => 2.94\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 67%|██████▋   | 4/6 [00:13<00:06,  3.25s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction(\n",
            "    assessment_answer='Assessed Text: Grug is coder who gather thinkings on code making.\\nAssessment Question: Does the assessed text have the same meaning as the gold_standard text provided?\\nGold Standard: \"this collection of thoughts on software development gathered by grug brain developer\"\\nAssessment Answer: Yes'\n",
            ")\n",
            "ARI 0 => 5.05\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 83%|████████▎ | 5/6 [00:19<00:04,  4.35s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction(\n",
            "    assessment_answer='Assessed Text: grug, brain developer, try collect learning into small, easy funny pages. not just for young grug, but for him too. as grug get older, forget important things like breakfast or pants.\\nAssessment Question: Does the assessed text have the same meaning as the gold_standard text provided?\\nGold Standard: \"grug brain developer try collect learns into small, easily digestible and funny page, not only for you, the young grug, but also for him because as grug brain developer get older he forget important things, like what had for breakfast or if put pants on\"\\nAssessment Answer: Yes'\n",
            ")\n",
            "ARI 22.95 => 6.98\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 6/6 [00:20<00:00,  3.48s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction(\n",
            "    assessment_answer='Assessment Answer: Yes'\n",
            ")\n",
            "ARI 0 => 0\n",
            "Bootstrapped 0 full traces after 6 examples in round 0.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# output evaluation\n",
        "from dspy.evaluate import Evaluate\n",
        "individual_metrics = [similarity_metric, ari_metric]\n",
        "for metric in individual_metrics:\n",
        "    evaluate = Evaluate(metric=metric, devset=train, num_threads=1, display_progress=True, display_table=5)\n",
        "    evaluate(optimized_cot)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "u9JxaIrmjKWb",
        "outputId": "4f378a58-0304-4e60-9f0e-26bb72a36cdf"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Average Metric: 0 / 1  (0.0):  17%|█▋        | 1/6 [00:03<00:19,  3.92s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction(\n",
            "    assessment_answer='Assessed Text: grug used to think big brain, but learn hard way not as smart as think\\nAssessment Question: Does the assessed text have the same meaning as the gold_standard text provided?\\nGold Standard: \"(note: grug once think big brained but learn hard way)\"\\nAssessment Answer: Yes'\n",
            ")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Average Metric: 0 / 2  (0.0):  33%|███▎      | 2/6 [00:04<00:08,  2.03s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction(\n",
            "    assessment_answer='Assessed Text: That text appears to already be in plain English and does not require translation.\\nAssessment Question: Does the assessed text have the same meaning as the gold_standard text provided?\\nGold Standard: \"is fine!\"\\nAssessment Answer: Yes'\n",
            ")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Average Metric: 0 / 3  (0.0):  50%|█████     | 3/6 [00:07<00:06,  2.19s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction(\n",
            "    assessment_answer='Assessed Text: \"Complexity bad.\"\\nAssessment Question: Does the assessed text have the same meaning as the gold_standard text provided?\\nGold Standard: \"complexity bad\"\\nAssessment Answer: Yes'\n",
            ")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Average Metric: 0 / 4  (0.0):  67%|██████▋   | 4/6 [00:08<00:03,  1.76s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction(\n",
            "    assessment_answer='Assessment Answer: Yes'\n",
            ")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Average Metric: 1 / 5  (20.0):  83%|████████▎ | 5/6 [00:09<00:01,  1.77s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction(\n",
            "    assessment_answer='Yes'\n",
            ")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Average Metric: 1 / 6  (16.7): 100%|██████████| 6/6 [00:11<00:00,  1.88s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction(\n",
            "    assessment_answer='Assessment Answer: Yes'\n",
            ")\n",
            "Average Metric: 1 / 6  (16.7%)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<pandas.io.formats.style.Styler at 0x7fc2c983ae90>"
            ],
            "text/html": [
              "<style type=\"text/css\">\n",
              "#T_34052 th {\n",
              "  text-align: left;\n",
              "}\n",
              "#T_34052 td {\n",
              "  text-align: left;\n",
              "}\n",
              "#T_34052_row0_col0, #T_34052_row0_col1, #T_34052_row0_col2, #T_34052_row0_col3, #T_34052_row0_col4, #T_34052_row1_col0, #T_34052_row1_col1, #T_34052_row1_col2, #T_34052_row1_col3, #T_34052_row1_col4, #T_34052_row2_col0, #T_34052_row2_col1, #T_34052_row2_col2, #T_34052_row2_col3, #T_34052_row2_col4, #T_34052_row3_col0, #T_34052_row3_col1, #T_34052_row3_col2, #T_34052_row3_col3, #T_34052_row3_col4, #T_34052_row4_col0, #T_34052_row4_col1, #T_34052_row4_col2, #T_34052_row4_col3, #T_34052_row4_col4 {\n",
              "  text-align: left;\n",
              "  white-space: pre-wrap;\n",
              "  word-wrap: break-word;\n",
              "  max-width: 400px;\n",
              "}\n",
              "</style>\n",
              "<table id=\"T_34052\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th class=\"blank level0\" >&nbsp;</th>\n",
              "      <th id=\"T_34052_level0_col0\" class=\"col_heading level0 col0\" >example_grug_text</th>\n",
              "      <th id=\"T_34052_level0_col1\" class=\"col_heading level0 col1\" >plain_english</th>\n",
              "      <th id=\"T_34052_level0_col2\" class=\"col_heading level0 col2\" >rationale</th>\n",
              "      <th id=\"T_34052_level0_col3\" class=\"col_heading level0 col3\" >pred_grug_text</th>\n",
              "      <th id=\"T_34052_level0_col4\" class=\"col_heading level0 col4\" >similarity_metric</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th id=\"T_34052_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
              "      <td id=\"T_34052_row0_col0\" class=\"data row0 col0\" >(note: grug once think big brained but learn hard way)</td>\n",
              "      <td id=\"T_34052_row0_col1\" class=\"data row0 col1\" >Grug used to think he was very intelligent, but he learned the hard way that he was not as smart as he thought.</td>\n",
              "      <td id=\"T_34052_row0_col2\" class=\"data row0 col2\" >realize that Grug is not as smart as he thought. We...</td>\n",
              "      <td id=\"T_34052_row0_col3\" class=\"data row0 col3\" >grug used to think big brain, but learn hard way not as smart as think</td>\n",
              "      <td id=\"T_34052_row0_col4\" class=\"data row0 col4\" >False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_34052_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
              "      <td id=\"T_34052_row1_col0\" class=\"data row1 col0\" >is fine!</td>\n",
              "      <td id=\"T_34052_row1_col1\" class=\"data row1 col1\" >That text appears to already be in plain English and does not require translation.</td>\n",
              "      <td id=\"T_34052_row1_col2\" class=\"data row1 col2\" >not translate this text. We simply leave it as is.</td>\n",
              "      <td id=\"T_34052_row1_col3\" class=\"data row1 col3\" >That text appears to already be in plain English and does not require translation.</td>\n",
              "      <td id=\"T_34052_row1_col4\" class=\"data row1 col4\" >False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_34052_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
              "      <td id=\"T_34052_row2_col0\" class=\"data row2 col0\" >complexity bad</td>\n",
              "      <td id=\"T_34052_row2_col1\" class=\"data row2 col1\" >\"Complexity is bad.\"</td>\n",
              "      <td id=\"T_34052_row2_col2\" class=\"data row2 col2\" >produce the Grug text. We need to simplify this statement and convey it in a straightforward manner.</td>\n",
              "      <td id=\"T_34052_row2_col3\" class=\"data row2 col3\" >\"Complexity bad.\"</td>\n",
              "      <td id=\"T_34052_row2_col4\" class=\"data row2 col4\" >False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_34052_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
              "      <td id=\"T_34052_row3_col0\" class=\"data row3 col0\" >this collection of thoughts on software development gathered by grug brain developer</td>\n",
              "      <td id=\"T_34052_row3_col1\" class=\"data row3 col1\" >Grug is a software developer who has collected thoughts on software development.</td>\n",
              "      <td id=\"T_34052_row3_col2\" class=\"data row3 col2\" >produce the grug_text. We will break down the sentence and translate each part into Grug text.</td>\n",
              "      <td id=\"T_34052_row3_col3\" class=\"data row3 col3\" >grug software developer collect thought on software developer</td>\n",
              "      <td id=\"T_34052_row3_col4\" class=\"data row3 col4\" >False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_34052_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
              "      <td id=\"T_34052_row4_col0\" class=\"data row4 col0\" >grug brain developer try collect learns into small, easily digestible and funny page, not only for you, the young grug, but also for him because...</td>\n",
              "      <td id=\"T_34052_row4_col1\" class=\"data row4 col1\" >Grug, as a brain developer, tries to collect learning into small, easily digestible, and funny pages. These are not only for you, the young Grug,...</td>\n",
              "      <td id=\"T_34052_row4_col2\" class=\"data row4 col2\" >produce the grug_text. We will break down the sentence and translate each part into Grug text.</td>\n",
              "      <td id=\"T_34052_row4_col3\" class=\"data row4 col3\" >grug brain developer try collect learning into small, easy digest, funny page. not only for you, young grug, but also for him. as grug, brain...</td>\n",
              "      <td id=\"T_34052_row4_col4\" class=\"data row4 col4\" >✔️ [True]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "                <div style='\n",
              "                    text-align: center;\n",
              "                    font-size: 16px;\n",
              "                    font-weight: bold;\n",
              "                    color: #555;\n",
              "                    margin: 10px 0;'>\n",
              "                    ... 1 more rows not displayed ...\n",
              "                </div>\n",
              "                "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Average Metric: 3 / 6  (50.0): 100%|██████████| 6/6 [00:00<00:00, 289.54it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ARI 0 => 0\n",
            "ARI -3.95 => 8.78\n",
            "ARI 0 => 17.25\n",
            "ARI 0 => 0\n",
            "ARI 22.95 => 10.78\n",
            "ARI 0 => 0\n",
            "Average Metric: 3 / 6  (50.0%)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<pandas.io.formats.style.Styler at 0x7fc2b0d69270>"
            ],
            "text/html": [
              "<style type=\"text/css\">\n",
              "#T_9ae6a th {\n",
              "  text-align: left;\n",
              "}\n",
              "#T_9ae6a td {\n",
              "  text-align: left;\n",
              "}\n",
              "#T_9ae6a_row0_col0, #T_9ae6a_row0_col1, #T_9ae6a_row0_col2, #T_9ae6a_row0_col3, #T_9ae6a_row0_col4, #T_9ae6a_row1_col0, #T_9ae6a_row1_col1, #T_9ae6a_row1_col2, #T_9ae6a_row1_col3, #T_9ae6a_row1_col4, #T_9ae6a_row2_col0, #T_9ae6a_row2_col1, #T_9ae6a_row2_col2, #T_9ae6a_row2_col3, #T_9ae6a_row2_col4, #T_9ae6a_row3_col0, #T_9ae6a_row3_col1, #T_9ae6a_row3_col2, #T_9ae6a_row3_col3, #T_9ae6a_row3_col4, #T_9ae6a_row4_col0, #T_9ae6a_row4_col1, #T_9ae6a_row4_col2, #T_9ae6a_row4_col3, #T_9ae6a_row4_col4 {\n",
              "  text-align: left;\n",
              "  white-space: pre-wrap;\n",
              "  word-wrap: break-word;\n",
              "  max-width: 400px;\n",
              "}\n",
              "</style>\n",
              "<table id=\"T_9ae6a\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th class=\"blank level0\" >&nbsp;</th>\n",
              "      <th id=\"T_9ae6a_level0_col0\" class=\"col_heading level0 col0\" >example_grug_text</th>\n",
              "      <th id=\"T_9ae6a_level0_col1\" class=\"col_heading level0 col1\" >plain_english</th>\n",
              "      <th id=\"T_9ae6a_level0_col2\" class=\"col_heading level0 col2\" >rationale</th>\n",
              "      <th id=\"T_9ae6a_level0_col3\" class=\"col_heading level0 col3\" >pred_grug_text</th>\n",
              "      <th id=\"T_9ae6a_level0_col4\" class=\"col_heading level0 col4\" >ari_metric</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th id=\"T_9ae6a_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
              "      <td id=\"T_9ae6a_row0_col0\" class=\"data row0 col0\" >(note: grug once think big brained but learn hard way)</td>\n",
              "      <td id=\"T_9ae6a_row0_col1\" class=\"data row0 col1\" >Grug used to think he was very intelligent, but he learned the hard way that he was not as smart as he thought.</td>\n",
              "      <td id=\"T_9ae6a_row0_col2\" class=\"data row0 col2\" >realize that Grug is not as smart as he thought. We...</td>\n",
              "      <td id=\"T_9ae6a_row0_col3\" class=\"data row0 col3\" >grug used to think big brain, but learn hard way not as smart as think</td>\n",
              "      <td id=\"T_9ae6a_row0_col4\" class=\"data row0 col4\" >✔️ [True]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_9ae6a_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
              "      <td id=\"T_9ae6a_row1_col0\" class=\"data row1 col0\" >is fine!</td>\n",
              "      <td id=\"T_9ae6a_row1_col1\" class=\"data row1 col1\" >That text appears to already be in plain English and does not require translation.</td>\n",
              "      <td id=\"T_9ae6a_row1_col2\" class=\"data row1 col2\" >not translate this text. We simply leave it as is.</td>\n",
              "      <td id=\"T_9ae6a_row1_col3\" class=\"data row1 col3\" >That text appears to already be in plain English and does not require translation.</td>\n",
              "      <td id=\"T_9ae6a_row1_col4\" class=\"data row1 col4\" >False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_9ae6a_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
              "      <td id=\"T_9ae6a_row2_col0\" class=\"data row2 col0\" >complexity bad</td>\n",
              "      <td id=\"T_9ae6a_row2_col1\" class=\"data row2 col1\" >\"Complexity is bad.\"</td>\n",
              "      <td id=\"T_9ae6a_row2_col2\" class=\"data row2 col2\" >produce the Grug text. We need to simplify this statement and convey it in a straightforward manner.</td>\n",
              "      <td id=\"T_9ae6a_row2_col3\" class=\"data row2 col3\" >\"Complexity bad.\"</td>\n",
              "      <td id=\"T_9ae6a_row2_col4\" class=\"data row2 col4\" >False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_9ae6a_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
              "      <td id=\"T_9ae6a_row3_col0\" class=\"data row3 col0\" >this collection of thoughts on software development gathered by grug brain developer</td>\n",
              "      <td id=\"T_9ae6a_row3_col1\" class=\"data row3 col1\" >Grug is a software developer who has collected thoughts on software development.</td>\n",
              "      <td id=\"T_9ae6a_row3_col2\" class=\"data row3 col2\" >produce the grug_text. We will break down the sentence and translate each part into Grug text.</td>\n",
              "      <td id=\"T_9ae6a_row3_col3\" class=\"data row3 col3\" >grug software developer collect thought on software developer</td>\n",
              "      <td id=\"T_9ae6a_row3_col4\" class=\"data row3 col4\" >✔️ [True]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_9ae6a_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
              "      <td id=\"T_9ae6a_row4_col0\" class=\"data row4 col0\" >grug brain developer try collect learns into small, easily digestible and funny page, not only for you, the young grug, but also for him because...</td>\n",
              "      <td id=\"T_9ae6a_row4_col1\" class=\"data row4 col1\" >Grug, as a brain developer, tries to collect learning into small, easily digestible, and funny pages. These are not only for you, the young Grug,...</td>\n",
              "      <td id=\"T_9ae6a_row4_col2\" class=\"data row4 col2\" >produce the grug_text. We will break down the sentence and translate each part into Grug text.</td>\n",
              "      <td id=\"T_9ae6a_row4_col3\" class=\"data row4 col3\" >grug brain developer try collect learning into small, easy digest, funny page. not only for you, young grug, but also for him. as grug, brain...</td>\n",
              "      <td id=\"T_9ae6a_row4_col4\" class=\"data row4 col4\" >False</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "                <div style='\n",
              "                    text-align: center;\n",
              "                    font-size: 16px;\n",
              "                    font-weight: bold;\n",
              "                    color: #555;\n",
              "                    margin: 10px 0;'>\n",
              "                    ... 1 more rows not displayed ...\n",
              "                </div>\n",
              "                "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# have a look\n",
        "optimized_cot.forward(\"You should not construct complex systems.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JXo14eBbjOdc",
        "outputId": "f39e5895-477f-4c7d-c4b5-4fca8d9c9639"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Prediction(\n",
              "    rationale='avoid confusion and keep things simple for Grug to understand.',\n",
              "    grug_text='no build complex system'\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# save the optimized model\n",
        "optimized_cot.save(path=\"/tmp/model.json\")\n"
      ],
      "metadata": {
        "id": "9mYTKDE5jSKi"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "1B9fn6y5jYl4",
        "outputId": "259ffec3-4da8-4cc5-bd65-dd5ca3c4f5e4"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (<ipython-input-3-4113a1f1792d>, line 3)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-3-4113a1f1792d>\"\u001b[0;36m, line \u001b[0;32m3\u001b[0m\n\u001b[0;31m    Summary about prompt engineering using Dspy.\u001b[0m\n\u001b[0m            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Summary\n",
        "\n",
        "Summary about prompt engineering using Dspy."
      ],
      "metadata": {
        "id": "8-kKLRTMjcpz"
      }
    }
  ]
}